{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import scipy.fftpack as fftpack\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from mfilter.regressions import *\n",
    "from mfilter.types import FrequencySamples, TimeSeries, FrequencySeries, TimesSamples\n",
    "from mfilter.filter import *\n",
    "from microlensing import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal detection with matched filter\n",
    "\n",
    "\n",
    "### Motivation\n",
    "* LIGO works on gravitational waves\n",
    "    * Perform a signal detection usign as impulse response a lot of templates.\n",
    "\n",
    "### Basic Idea\n",
    "* Introduce Matched filter and with the use of Fourier Transform, compute a fast optimiization of what would be the optimal Signal to Noise Ratio (SNR) of any input observation data with an impulse response filter\n",
    "\n",
    "Where te linear filter is: \n",
    "\n",
    "$$ (x | h) = [x * h](t_0) = \\int_{-\\infty}^{\\infty} \\tilde{x}(f)\\tilde{h}^{*}(f) e^{2 \\pi i f t_0} df$$\n",
    "    \n",
    "### Steps\n",
    "* Change from evenly sampled data to un-evenly sampled data and with this use NFFT or regression.\n",
    "    * for the Direct Transform:\n",
    "    $$ F^{-1}[\\tilde{x}(f)] = x(t) = 2\\int_{0}^{\\infty} \\tilde{x}(f)e^{2 \\pi i f t} df \\longrightarrow x_j = 2 \\sum_{n = 0}^{N} \\tilde{x}[k] e^{2\\pi i \\Delta f k t_n} $$\n",
    "    * And the Adjoint Transform:\n",
    "$$ F[x(t)] = \\tilde{x}(f) = 2\\int_{0}^{\\infty} x(t) e^{-2\\pi i f t} dt  \\longrightarrow \\tilde{x}_k = 2 \\sum_{n = 0}^{N} x[n] e^{-2\\pi i \\Delta f k t_n}$$\n",
    "\n",
    "\n",
    "* Then in un-evenly space compute:\n",
    "    * Average PSD of the noise usign the data imput (Lomb-Welch periodogram) using Astropy Implementation\n",
    "    * Estimate Inverse Fourier Transform using some iterative method. In this case is implemented for Linear Regressions and used mostly the Ridge Algorithm implemented in sklearn which minimize:\n",
    "    \n",
    "    $$ \\underset{\\omega}{\\text{ min }} || X\\omega - y ||^{2}_{2} + \\alpha ||\\omega||_{2}^{2}$$\n",
    "    \n",
    "    Where $\\omega$ is the penalty introduced to avoid singular matrix.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'coef_init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-22e327b8b34d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mhtilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frequencyseries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score of the regresion of template\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/Match_filter/mfilter/types/timeseries.py\u001b[0m in \u001b[0;36mto_frequencyseries\u001b[0;34m(self, method, window, scale, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m\"nfft\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/Match_filter/mfilter/types/timeseries.py\u001b[0m in \u001b[0;36mregression\u001b[0;34m(self, series, regressor, scale)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 return FrequencySeries(regressor.get_ft(series),\n\u001b[0m\u001b[1;32m    215\u001b[0m                                        frequency_grid=regressor.frequency, epoch=self.epoch)\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/Match_filter/mfilter/regressions/regressors.py\u001b[0m in \u001b[0;36mget_ft\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cast_into_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/Match_filter/mfilter/regressions/regressors.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplited_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;31m# self._reg.fit(self._dict.splited_matrix, to_fit)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'coef_init'"
     ]
    }
   ],
   "source": [
    "delta_t = 0.8 / 2\n",
    "n = 60*2\n",
    "struct = \"slight\"\n",
    "times = TimesSamples(n=n, delta=delta_t, struct=struct)\n",
    "\n",
    "def eval(U0, t0, tE, fs, times):\n",
    "    \"\"\"Evaluate microlens event at given time array, return magnitude difference\n",
    "    Function taken from Microlensing implementation\"\"\"\n",
    "    u = np.sqrt(U0**2 + ((times - t0)  /tE )**2)\n",
    "    A = (u**2 + 2.) / (u * np.sqrt(u**2 + 3))\n",
    "    dm = - 2.5 * np.log10(fs*(A - 1) + 1)\n",
    "    return dm\n",
    "\n",
    "t0 = min(times) + times.duration/2\n",
    "tE = 0.8 \n",
    "U0 = 3 #1.3 \n",
    "fs = 24 #16.9\n",
    "val = eval(U0, times[n//5], tE, fs, times)\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "noise_data = np.random.normal(0, 0.6, n)\n",
    "data = val + noise_data\n",
    "\n",
    "tE = 0.8 \n",
    "U0 = 3 #1.3 \n",
    "fs = 24 #16.9\n",
    "val = eval(U0, t0, tE, fs, times)\n",
    "\n",
    "tE = 1\n",
    "U0 = 4.3\n",
    "fs = 22\n",
    "val2 = eval(U0, t0, tE, fs, times)\n",
    "\n",
    "val = TimeSeries(val, times=times)\n",
    "val2 = TimeSeries(val2, times=times)\n",
    "data = TimeSeries(data, times=times)\n",
    "noise = TimeSeries(np.random.normal(0, 0.6, n), times=times)\n",
    "\n",
    "## Fourier Transform\n",
    "\n",
    "# define freq. grid\n",
    "nyq = 1 / (2 * delta_t)\n",
    "samples_per_peak = 5\n",
    "df = 1 / times.duration / samples_per_peak\n",
    "freqs = FrequencySamples(input_time=times,\n",
    "                        minimum_frequency=samples_per_peak * df,\n",
    "                        maximum_frequency=nyq + samples_per_peak * df,\n",
    "                        samples_per_peak=samples_per_peak)\n",
    "# reg = LassoRegression(alpha=10**(-3), phi=F)\n",
    "reg = RidgeRegression(alpha=10**(1), phi=F)\n",
    "reg.create_dict(frequency=freqs, times=times)\n",
    "htilde = val.to_frequencyseries(reg=reg)\n",
    "print(\"score of the regresion of template\", reg.score(val))\n",
    "reg.reset()\n",
    "htilde2 = val2.to_frequencyseries(reg=reg)\n",
    "print(\"score of the regresion of template2\", reg.score(val2))\n",
    "reg.reset()\n",
    "ntilde = noise.to_frequencyseries(reg=reg)\n",
    "reg.reset()\n",
    "stilde = data.to_frequencyseries(reg=reg)\n",
    "print(\"score of the regresion of data\", reg.score(data))\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(16, 3))\n",
    "ax1.plot(times, data, 'r', label='data')\n",
    "# ax1.plot(times, val, 'k', label=\"template\")\n",
    "# ax1.plot(times, val2, 'g', label=\"template2\")\n",
    "ax1.plot(times, reg.predict(reg.dict), 'b--', label=\"reconstructed data\")\n",
    "ax1.plot(times, stilde.to_timeseries(reg=reg).real, label=\"backward data\")\n",
    "ax1.set_title(\"TimeSeries samples\", fontsize=18)\n",
    "ax1.set_xlabel(\"Times (seg)\", fontsize=18)\n",
    "ax1.legend(fontsize=16)\n",
    "\n",
    "ax2.plot(freqs, abs(stilde), 'r', label=\"data\")\n",
    "# ax2.plot(freqs, abs(htilde), 'k', label=\"template\")\n",
    "# ax2.plot(freqs, abs(htilde2), 'k', label=\"template2\")\n",
    "ax2.set_title(\"Frequency Series samples\", fontsize=18)\n",
    "ax2.set_xlabel(\"Frequency (Hz)\", fontsize=18)\n",
    "# ax2.legend(fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(times, val, 'k', label=\"template\")\n",
    "plt.plot(times, val2, 'g', label=\"template2\")\n",
    "plt.xlabel(\"Times (seg)\", fontsize=18)\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psd = data.psd(freqs)\n",
    "psd0 = noise.psd(freqs)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(16, 4))\n",
    "ax1.plot(freqs, psd, 'o--', label=\"PSD of data\")\n",
    "# ax1.plot(freqs, psd0, 'go--', label=\"PSD of template\")\n",
    "ax1.set_title(\"direct PSD with lomb-scarble\",fontsize=18)\n",
    "ax1.set_xlabel(\"Frequency (Hz)\", fontsize=18)\n",
    "ax1.legend(fontsize=18)\n",
    "\n",
    "def average_psd(segment_duration, oversampling_rate, timeSeries, freqs):\n",
    "    N = timeSeries.duration / segment_duration\n",
    "    n_seg = (int(N) - 1) / (1-oversampling_rate)\n",
    "#     print(n_seg)\n",
    "    t = 0\n",
    "    psd = FrequencySeries(np.zeros(len(freqs)), frequency_grid=freqs, epoch=timeSeries.epoch)\n",
    "    counter = 0\n",
    "    while t < timeSeries.duration - segment_duration:\n",
    "        aux_timeseries = timeSeries.get_time_slice(t, t+segment_duration)\n",
    "        window = signal.windows.hann(len(aux_timeseries))\n",
    "        aux_timeseries *= window\n",
    "        W = (window ** 2).sum() / len(window)\n",
    "#         W = 1\n",
    "        psd += (aux_timeseries.psd(freqs) / W)\n",
    "        t += (1-oversampling_rate) * segment_duration\n",
    "        counter += 1\n",
    "#     print(\"counter is: \", counter)\n",
    "    aux_timeseries = timeSeries.get_time_slice(timeSeries.duration - segment_duration, timeSeries.duration)\n",
    "    window = signal.windows.hann(len(aux_timeseries))\n",
    "    aux_timeseries *= window\n",
    "    W = (window ** 2).sum() / len(window)\n",
    "#     W = 1\n",
    "    psd += (aux_timeseries.psd(freqs) / W)\n",
    "    psd /= (counter+1)\n",
    "    \n",
    "    return psd\n",
    "\n",
    "seg_dur = times.duration//1\n",
    "psd = average_psd(seg_dur, 0.5, data, freqs)\n",
    "psd_test = average_psd(seg_dur, 0.5, TimeSeries(noise_data, times=times), freqs)\n",
    "# np.random.seed(1)\n",
    "# noise_data = np.random.normal(0, 0.2, n)\n",
    "psd0 = average_psd(seg_dur, 0.5, noise, freqs)\n",
    "\n",
    "\n",
    "ax2.plot(freqs, psd, label=\"estimated PSD of noise\")\n",
    "ax2.plot(freqs, psd_test, label=\"true PSD of noise\")\n",
    "# ax2.plot(freqs, psd0, 'g', label=\"PSD of template\")\n",
    "ax2.set_title(\"Average Lomg-Scargle PSD\", fontsize=18)\n",
    "ax2.set_xlabel(\"Frequency (Hz)\", fontsize=18)\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Generalize Matched Filter for colored noise instead of Withe Noise using Whitening Filter, this still need a Wide Sense Stationary noise.\n",
    "\n",
    "$$ S_n(f) |\\tilde{h}_w(f)|^{2} = S_{w}(f) \\text{ such that } \\sigma^{2} = \\int_{-\\infty}^{\\infty} S_w(f) df$$\n",
    "\n",
    "where $S_n$ is the PSD of the noise data, $\\tilde{h}_w$ is the Fourier Transform of the whitening filter and $S_w$ is the PSD of the Whitening filter. This directly leads to:\n",
    "\n",
    "$$ |\\tilde{h}_w(f)|^{2} =\\frac{S_w(f)}{S_n(f)} = \\frac{N_0}{S_n(f)}$$\n",
    "\n",
    "this produce a Whitened Matched Filter:\n",
    "\n",
    "$$ (x|h) = N_0 \\int_{-\\infty}^{\\infty} \\frac{\\tilde{x}(f)\\tilde{h}^{*}(f)}{S_n(f)} e^{2 \\pi i f t_0} df $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normalize SNR using Variance of the convolved output\n",
    "\n",
    "$$ \\sigma_h^{2} = N_0^{2}\\int_{-\\infty}^{\\infty} \\frac{|\\tilde{h}^{*}(f)|^{2}}{S_n(f)}df$$\n",
    "\n",
    "leads to:\n",
    "\n",
    "$$ SNR = \\frac{(x|h)}{\\sigma_h} = \\frac{\\int_{-\\infty}^{\\infty} \\frac{\\tilde{x}(f)\\tilde{h}^{*}(f)}{S_n(f)} e^{2\\pi i f t_0} df}{\\int_{-\\infty}^{\\infty}\\frac{|\\tilde{h}^{*}(f)|^{2}}{S_n(f)}df}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = matched_filter(htilde, stilde, psd=None, reg=reg, \n",
    "                     times=times, unitary_energy=True)\n",
    "snr2 = matched_filter(htilde2, stilde, psd=None, reg=reg, \n",
    "                      times=times, unitary_energy=True)\n",
    "\n",
    "snr0 = matched_filter(htilde, ntilde, psd=None, reg=reg, \n",
    "                      times=times, unitary_energy=True)\n",
    "\n",
    "fig, [ax, ax0] = plt.subplots(1, 2, figsize=(16, 4), sharey=True)\n",
    "ax.plot(times, snr.real, 'g', label=\"data with real temp.\")\n",
    "ax.plot(times, snr2.real, label=\"data with another temp.\")\n",
    "# ax.plot(times, snr0.real)\n",
    "ax.set_xlabel(\"times offset (sec)\", fontsize=18)\n",
    "ax.set_title(\"SNR\", fontsize=18)\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "ax0.plot(times, snr0.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finnaly define the choose of a good detection\n",
    "    * Over a first threshold, selecte candidates for detections\n",
    "    * Do a Chi-Square Veto in order to discriminate bad detections\n",
    "    * Try with a hypothesis Test to check probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the chi statistic\n",
    "\n",
    "def power_chisq(htilde, stilde, num_bins, psd, reg, times):\n",
    "    \n",
    "    bins = power_chisq_bins(htilde, num_bins, psd)\n",
    "    \n",
    "    \n",
    "    snr, corr, norm = matched_filter_core(htilde, stilde, psd=psd, reg=reg, times=times)\n",
    "    \n",
    "    return power_chisq_from_precomputed(corr, snr, norm, bins, reg), len(bins)\n",
    "\n",
    "def power_chisq_bins(htilde, num_bins, psd):\n",
    "    sigma_vec = sigmasq_series(htilde, psd)\n",
    "    \n",
    "    return power_chisq_bins_from_sigmasq_series(sigma_vec, num_bins)\n",
    "\n",
    "def sigmasq_series(htilde, psd):\n",
    "    autocorr = htilde.conj() * htilde\n",
    "    if psd is not None:\n",
    "        autocorr /= psd\n",
    "    return autocorr.cumsum()\n",
    "\n",
    "def power_chisq_bins_from_sigmasq_series(sigma_vec, num_bins):\n",
    "    sigmasq = sigma_vec[len(sigma_vec)-2]\n",
    "    edge_vec = np.arange(0, num_bins) * sigmasq / num_bins\n",
    "    bins = np.searchsorted(sigma_vec, edge_vec, side='right')\n",
    "    bins = np.append(bins, len(sigma_vec) - 1)\n",
    "    bins = np.unique(bins)\n",
    "    if len(bins) != num_bins + 1:\n",
    "        print(\"using {} bins instead of {}\".format(len(bins), num_bins))\n",
    "    return bins\n",
    "\n",
    "def power_chisq_from_precomputed(corr, snr, norm, bins, reg):\n",
    "    qtilde = FrequencySeries(np.zeros(len(corr)), frequency_grid=corr.frequency_object, dtype=corr.dtype,\n",
    "                            epoch=corr.epoch)\n",
    "    chisq = TimeSeries(np.zeros(len(snr)), times=snr.times, dtype=snr.dtype, epoch=snr.epoch)\n",
    "    num_bins = len(bins) - 1\n",
    "    \n",
    "    for j in range(num_bins):\n",
    "        k_min = int(bins[j])\n",
    "        k_max = int(bins[j+1])\n",
    "        qtilde[k_min:k_max] = corr[k_min:k_max]\n",
    "        q = qtilde.to_timeseries(reg=reg)\n",
    "        qtilde.fill(0)\n",
    "        chisq += q.squared_norm()\n",
    "        \n",
    "    chisq = (chisq * num_bins - snr.squared_norm()) * (norm ** 2)\n",
    "    chisq = TimeSeries(chisq, times=snr.times, epoch=snr.epoch)\n",
    "    return chisq\n",
    "\n",
    "divs = 40\n",
    "chsq, n_bins = power_chisq(htilde, stilde, divs, None, reg, times)\n",
    "chsq /= (n_bins * 2) - 2\n",
    "\n",
    "chsq2, n_bins = power_chisq(htilde2, stilde, divs, None, reg, times)\n",
    "chsq2 /= (n_bins * 2) - 2\n",
    "\n",
    "chsq0, n_bins = power_chisq(htilde2, ntilde, divs, None, reg, times)\n",
    "chsq0 /= (n_bins * 2) - 2\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(16, 3))\n",
    "ax1.plot(times - times[n//2], np.roll(chsq.real, n//2), 'g', label=\"data with temp1\")\n",
    "ax1.plot(times - times[n//2], np.roll(chsq2.real, n//2), 'b', label=\"data with temp2\")\n",
    "ax1.set_title(\"chi-square veto\")\n",
    "ax1.legend()\n",
    "ax2.plot(times - times[n//2], np.roll(chsq0.real, n//2))\n",
    "\n",
    "def weighted_snr(snr, chisq):\n",
    "    for i in range(len(chisq)):\n",
    "        if chisq[i] > 1:\n",
    "            snr[i] /= ((1 + chisq[i]**(3))/2.0)**(1.0/6)\n",
    "        \n",
    "    return snr\n",
    "\n",
    "w_snr = weighted_snr(snr, chsq)\n",
    "w_snr2 = weighted_snr(snr2, chsq2)\n",
    "w_snr0 = weighted_snr(snr0, chsq0)\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(16, 3), sharey=True)\n",
    "\n",
    "ax1.plot(times - times[n//2], (np.roll(w_snr, n//2)).real, 'g', \n",
    "         label=\"data with real temp.\")\n",
    "ax1.plot(times - times[n//2], (np.roll(w_snr2, n//2)).real, 'b', \n",
    "         label=\"data with another temp\", alpha=0.5)\n",
    "ax1.legend()\n",
    "ax1.set_title(\"SNR after chi-square veto\")\n",
    "ax2.plot(times - times[n//2], (np.roll(w_snr0, n//2)).real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "* The method it is highly dependent on how the sample of the data is and this lead to results that cannot differenciate very similar templates, just shows if there is a detection or not, doesn't say exactply what type of detections it is.\n",
    "\n",
    "### Future works\n",
    "* The actual algorithm need to be recomputed every time a new observation is maed which is very expensive and shoulb be optimized\n",
    "* The input data could be received in different bands which means different Time Series with no necessary same epoch, this introduce the necessity of perform a multi-band signal detection using matched filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pynfft import NFFT, Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times2 = TimesSamples(initial_array=np.linspace(0, 10, 100))\n",
    "dt = 0.1\n",
    "freq = 1 / dt\n",
    "print(freq)\n",
    "window = signal.windows.hann(len(times2))\n",
    "window = 1\n",
    "nnoise = np.random.normal(0, 0.5, len(times2))\n",
    "data = TimeSeries((np.sin(2 * np.pi * freq * times2) + nnoise) * window, times=times2)\n",
    "temp = TimeSeries(np.sin(2 * np.pi * freq * times2) * window, times=times2) \n",
    "noise = TimeSeries(nnoise * window, times=times2)\n",
    "plt.plot(times2, data)\n",
    "plt.plot(times2, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times2, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan = NFFT(len(times2), len(times2))\n",
    "# plan.x = times2.value\n",
    "# print(plan.N_total)\n",
    "# plan.precompute()\n",
    "\n",
    "plan2 = NFFT(len(times2), len(times2))\n",
    "plan2.x = times2.value\n",
    "print(plan2.N_total)\n",
    "plan2.precompute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjoint transform\n",
    "# plan.f = data.value\n",
    "# f_hat = plan.adjoint()\n",
    "\n",
    "plan2.f = noise.value\n",
    "f_hat2 = plan2.adjoint()\n",
    "\n",
    "# inverse\n",
    "# infft = Solver(plan)\n",
    "# infft.y = temp.value\n",
    "# infft.before_loop() \n",
    "niter = 100 # set number of iterations to 10\n",
    "# for iiter in range(niter):\n",
    "#     infft.loop_one_step()\n",
    "    \n",
    "infft2 = Solver(plan2)\n",
    "infft2.y = noise.value\n",
    "infft2.before_loop() \n",
    "for iiter in range(niter):\n",
    "    infft2.loop_one_step()\n",
    "    \n",
    "    \n",
    "# direct (reconstruct)\n",
    "# plan.f_hat = infft.f_hat_iter\n",
    "# f = plan.trafo()\n",
    "\n",
    "plan2.f_hat = infft2.f_hat_iter\n",
    "f2 = plan2.trafo()\n",
    "\n",
    "    \n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(16, 3))\n",
    "N_f = len(infft2.f_hat_iter)\n",
    "freqs = np.fft.fftfreq(N_f)\n",
    "# freqs = np.arange(N_f)\n",
    "# ax1.plot(freqs, abs(infft.f_hat_iter), 'b.')\n",
    "ax1.plot(freqs, abs(infft2.f_hat_iter), 'r')\n",
    "# ax2.plot(infft.r_iter.real, 'g')\n",
    "# print(len(infft.r_iter.real))\n",
    "# print(infft.f_hat_iter)  # current solution\n",
    "# print(infft.r_iter)  # current residuals\n",
    "\n",
    "# reconstruct\n",
    "# plt.figure()\n",
    "# plt.plot(times2, temp, 'b')\n",
    "# plt.plot(times2, f, 'r')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(times2, noise, 'b')\n",
    "plt.plot(times2, f2, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.49, 0.49, 100)\n",
    "Nfreq = 200\n",
    "\n",
    "plan = NFFT(Nfreq, len(x))\n",
    "plan.x = x\n",
    "plan.precompute()\n",
    "np.random.seed(147)\n",
    "plan.f = np.random.randn(100)\n",
    "\n",
    "result1 = plan.adjoint()\n",
    "saved = result1.copy()\n",
    "\n",
    "plan2 = NFFT(Nfreq, len(x))\n",
    "plan2.x = x\n",
    "plan2.precompute()\n",
    "np.random.seed(147)\n",
    "plan2.f = np.random.randn(100)\n",
    "plan2.adjoint()\n",
    "\n",
    "np.allclose(result1, saved) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times, f.real)\n",
    "plt.plot(times, f.imag, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(infft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
